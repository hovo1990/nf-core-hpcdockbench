

// -- * Enable singularity stuff
singularity.enabled     = true
singularity.autoMounts  = true
conda.enabled           = false
docker.enabled          = false
podman.enabled          = false
shifter.enabled         = false
charliecloud.enabled    = false
apptainer.enabled       = false


//-- * https://www.sdsc.edu/support/user_guides/expanse.html
//-- * max running jobs on shared is 4096 for queue
executor {
    // -- * works queueSize = 30
    queueSize = 300
}


//-- ! This makes sure that $HOME folder is not being used, and Expanse users won't be angry at you.
// workDir = "${params.output_workdir}"



process {
    executor = 'slurm'
    maxRetries = 1
    pollInterval = '2 min'
    queueStatInterval = '5 min'
    submitRateLimit = '6/1min'
    retry.maxAttempts = 1

    clusterOptions = ' --export=ALL --nodes=1 --ntasks-per-node=1 -t 09:00:00 -A $params.project '

    scratch = '/tmp/$USER/job_$SLURM_JOB_ID'

    //-- * This makes processes associated with low_cpu label to use only 1 core and 512MB ram

    //-- * Will run on AMD epyc 128 core, 256 gb ram
    withLabel:gpu {
        cpus = 10
        memory = '72 GB'
        time = '1 h'
        queue = 'gpu-compute'
        clusterOptions = " --export=ALL --nodes=1  --ntasks-per-node=1    --gpus=1 --exclude=armenia-n[433-594]"


    }

    //-- * Will run on AMD EPYC 128 core cpu nodes, 256 GB RAM, total 768 nodes
    withLabel:cpu_from_gpu{
        cpus = 16
        memory = '64 GB'
        time = '1 h'
        queue = 'compute'
        clusterOptions = " --export=ALL --nodes=1  --ntasks-per-node=1 --exclude=armenia-n[433-594]"


    }

    withLabel:process_low{
        cpus = 4
        memory = '6 GB'
        time = '2 h'
        queue = 'compute'
        clusterOptions = " --export=ALL --nodes=1  --ntasks-per-node=1 --exclude=armenia-n[433-594]"


    }

    withLabel:cpu_task {
        // errorStrategy = 'retry'
        memory = {8.GB * task.attempt}
        cpus = 8
        time = {2000.min * task.attempt}
        // maxRetries = { task.exitStatus >= 100 ? 4 : 1 }
        queue = 'compute'
        clusterOptions = " --export=ALL --nodes=1  --ntasks-per-node=1  --exclude=armenia-n[433-594] "
        beforeScript = 'hostname; echo "Wait random 25 secs"; sleep $((5 + RANDOM % 20))'
    }


    withLabel:low_cpu{
        // errorStrategy = 'retry'
        memory = {4.GB * task.attempt}
        cpus = 4
        time = {2000.min * task.attempt}
        // maxRetries = { task.exitStatus >= 100 ? 4 : 1 }
        queue = 'compute'
        clusterOptions = " --export=ALL --nodes=1  --ntasks-per-node=1  --exclude=armenia-n[433-594] "
        beforeScript = 'hostname; echo "Wait random 25 secs"; sleep $((5 + RANDOM % 20))'
    }

    withLabel:low_cpu_debug{
        memory = {2.GB * task.attempt}
        cpus = 2
        time = {2000.min * task.attempt}
        // maxRetries = { task.exitStatus >= 100 ? 4 : 1 }
        queue = 'compute'
        clusterOptions = " --export=ALL --nodes=1  --ntasks-per-node=1  --exclude=armenia-n[433-594] "
        beforeScript = 'hostname; echo "Wait random 25 secs"; sleep $((5 + RANDOM % 20))'
    }


    withLabel:very_low_cpu_debug{
        memory = {1.GB * task.attempt}
        cpus = 1
        time = {2000.min * task.attempt}
        // maxRetries = { task.exitStatus >= 100 ? 4 : 1 }
        queue = 'compute'
        clusterOptions = " --export=ALL --nodes=1  --ntasks-per-node=1  --exclude=armenia-n[433-594] "
        beforeScript = 'hostname; echo "Wait random 25 secs"; sleep $((5 + RANDOM % 20))'
    }



}
timeline.enabled = true
report.enabled = true
report.overwrite = true